{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reuters-21578 Text Categorization\n",
    "The goal of this section is to implement a machine learning model that correctly classifies a series of documents in the Reuters-21578 corpus. Each document's \"BODY\" and \"TITLE\" is used to predict the overall category, or \"TOPIC\", of the document.\n",
    "\n",
    "First, the frequency of top words in each document's \"BODY\" and \"TITLE\" is calculated and used to create a sparse matrix of features. The most popular \"TOPIC\" for each document comprises the list of labels\n",
    "\n",
    "The K-Means Clustering adds more value to analyzing the top words and groups them into 135 clusters, reflecting the 135 potential \"TOPICS\".\n",
    "\n",
    "The Naive Bayes algorithm uses two approaches to classify: SkLearn randomized the test_train_split and \"LEWISSPLIT\" predetermined split, which was used in the TOIS located in the Resources directory.\n",
    " - Using the SkLearn split, \"BODY\" accurately classfies 86% and \"TITLE\" accurately classifies about 87%.\n",
    " - Using the \"LEWISSPLIT\", \"BODY\" classifies about 77% while \"TITLE\" produces an accuracy of about 75%. These scores are lower than the SkLearn split, but are still higher than the Probabilistic Bayes model described in the paper, which produced an overall accuracy of 65%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency of Top Words\n",
    "This section determines the frequencies of top words in the entire corpus. The words are taken from either the text \"BODY\" or \"TITLE\".\n",
    "\n",
    "Words that do not have an impact on the overall categorization of the article, or stopwords, such as \"and\" and \"the\" are removed from the list of words. A sparse matrix is created for the features and a separate list is created with a single topic for each document ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from yellowbrick.text.freqdist import FreqDistVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = frozenset({'reuter', 'said'})\n",
    "english_stopwords = stop_words.ENGLISH_STOP_WORDS\n",
    "stopwords = stopwords.union(english_stopwords)\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = stopwords)\n",
    "\n",
    "def graph_frequencies(matrix, features):\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    visualizer = FreqDistVisualizer(features=features, ax=ax)\n",
    "    visualizer.fit(matrix)\n",
    "    visualizer.poof()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a sparse matrix and populates features for BODY and TITLE\n",
    "with open('body_no_null.csv') as f:\n",
    "    bodyContent = f.readlines()\n",
    "bodyData = [x.split(',')[2] for x in bodyContent]\n",
    "bodyMatrix = vectorizer.fit_transform(bodyData)\n",
    "bodyFeatures = vectorizer.get_feature_names()\n",
    "graph_frequencies(bodyMatrix, bodyFeatures)\n",
    "\n",
    "with open('title_no_null.csv') as f:\n",
    "    titleContent = f.readlines()\n",
    "titleData = [x.split(',')[2] for x in titleContent]\n",
    "titleMatrix = vectorizer.fit_transform(titleData)\n",
    "titleFeatures = vectorizer.get_feature_names()\n",
    "graph_frequencies(titleMatrix, titleFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array of all topics according to their document ID\n",
    "def get_labels(file):\n",
    "    with open(file) as f:\n",
    "        labelsContent = f.readlines()\n",
    "    labels = [x.split(',')[2] for x in labelsContent]\n",
    "    return labels\n",
    "\n",
    "labels = get_labels('topics_popular.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates a sparse matrix and populates the \"BODY\" and \"TITLE\" features according to the \"LEWISSPLIT\" training or test tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_files/body_train.csv') as f:\n",
    "    bodyTrainContent = f.readlines()\n",
    "bodyTrainData = [x.split(',')[2] for x in bodyTrainContent]\n",
    "bodyTrainMatrix = vectorizer.transform(bodyTrainData)\n",
    "bodyTrainFeatures = vectorizer.get_feature_names()\n",
    "#graph_frequencies(bodyTrainMatrix, bodyTrainFeatures)\n",
    "\n",
    "with open('testing_files/body_test.csv') as f:\n",
    "    bodyTestContent = f.readlines()\n",
    "bodyTestData = [x.split(',')[2] for x in bodyTestContent]\n",
    "bodyTestMatrix = vectorizer.transform(bodyTestData)\n",
    "bodyTestFeatures = vectorizer.get_feature_names()\n",
    "#graph_frequencies(bodyTestMatrix, bodyTestFeatures)\n",
    "\n",
    "with open('training_files/title_train.csv') as f:\n",
    "    titleTrainContent = f.readlines()\n",
    "titleTrainData = [x.split(',')[2] for x in titleTrainContent]\n",
    "titleTrainMatrix = vectorizer.transform(titleTrainData)\n",
    "titleTrainFeatures = vectorizer.get_feature_names()\n",
    "#graph_frequencies(titleTrainMatrix, titleTrainFeatures)\n",
    "\n",
    "with open('testing_files/title_test.csv') as f:\n",
    "    titleTestContent = f.readlines()\n",
    "titleTestData = [x.split(',')[2] for x in titleTestContent]\n",
    "titleTestMatrix = vectorizer.transform(titleTestData)\n",
    "titleTestFeatures = vectorizer.get_feature_names()\n",
    "#graph_frequencies(titleTestMatrix, titleTestFeatures)\n",
    "\n",
    "labelsTrain = get_labels('training_files/topics_train.csv')\n",
    "labelsTest = get_labels('testing_files/topics_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "Since there are 135 potential \"TOPICS\", there are 135 clusters with the top word frequencies. The first 5 clusters are displayed below with each run producing a different subset of words for each cluster. This analysis is a useful sanity check to identify that words that are grouped together align with predetermined topics located in topics_popular.csv\n",
    "\n",
    "Modified from: https://github.disney.com/JORDC054/twitter-friend-clusters/blob/master/twitter%20techvive%202018.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "mbk = MiniBatchKMeans(init='k-means++', n_clusters=135, batch_size=2500, n_init=10, max_no_improvements=10, verbose=0)\n",
    "\n",
    "mbk.fit(bodyMatrix) #titleMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clusters = mbk.labels_.tolist()\n",
    "\n",
    "frame = pd.DataFrame({'cluster' : clusters}, index = [clusters], columns = ['cluster'])\n",
    "#frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top words per cluster:\")\n",
    "print()\n",
    "\n",
    "#sort cluster centroids by proximity to centroid\n",
    "order_centroids = mbk.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Cluster %d had top words:\" % i, end= ' ')\n",
    "    for ind in order_centroids[i, :30]:\n",
    "        print(bodyFeaturs[ind], end = ', ') #change to titleFeatures to view \"TITLE\" top words\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "The following code splits the data into train and test sections with the features determine dby the sparse matrix and labels created above.\n",
    "\n",
    "Using SKlearn train_test_split: \"BODY\" produces an accuracy of about 86% and \"TITLE\" produces an accuracy of about 87%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general naive bayes on entire corpus\n",
    "def naive_bayes(matrix, labels, test, test_labels, curFeature):\n",
    "    #initialize classifier\n",
    "    mnb = MultinomialNB()\n",
    "    model = mnb.fit(matrix, labels)\n",
    "    preds = mnb.predict(test)\n",
    "    \n",
    "    #evaluate accuracy\n",
    "    print(\"Probability Prediction (\", curFeature, \"):\")\n",
    "    print(mnb.predict_proba(test))\n",
    "    print()\n",
    "    print(\"Confusion Matrix (\", curFeature, \"):\")\n",
    "    print(confusion_matrix(test_labels, preds))\n",
    "    print()\n",
    "    print(\"Classification Report (\", curFeature, \"):\")\n",
    "    print(classification_report(test_labels, preds))\n",
    "    print()\n",
    "    print(\"Accuracy Score (\", curFeature, \"):\")\n",
    "    print(accuracy_score(test_labels, preds)*100, \"%\")\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training and testing data\n",
    "trainBody, testBody, trainBody_labels, testBody_labels = train_test_split(bodyMatrix, labels, test_size = 0.33, random_state=42)\n",
    "trainTitle, testTitle, trainTitle_labels, testTitle_labels = train_test_split(titleMatrix, labels, test_size = 0.33, random_state=42)\n",
    "\n",
    "naive_bayes(trainBody, trainBody_labels, testBody, testBody_labels, \"BODY\")\n",
    "naive_bayes(trainTitle, trainTitle_labels, testTitle, testTitle_labels, \"TITLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the LEWISSPLIT training/test data\n",
    "naive_bayes(bodyTrainMatrix, labelsTrain, bodyTestMatrix, labelsTest, \"BODY\")\n",
    "naive_bayes(titleTrainMatrix, labelsTrain, titleTrainMatrix, labelsTest, \"TITLE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
